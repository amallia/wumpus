/**
 * Copyright (C) 2005 Stefan Buettcher. All rights reserved.
 * This is free software with ABSOLUTELY NO WARRANTY.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA
 * 02111-1307, USA
 **/

/**
 * This application takes a set of input indices INPUT_1 .. INPUT_N and creates
 * two output indices OUTPUT_LERGE and OUTPUT_SMALL. The OUTPUT_LARGE index is
 * a full index, containing all the information stored in the input indices.
 * OUTPUT_SMALL is a smaller index, containing impact-ordered information about
 * the N most frequent terms (for each, the top K documents are kept).
 *
 * Syntax:  combine_indices INPUT_1 .. INPUT_N OUTPUT_LARGE OUTPUT_SMALL N=n [K=k] [EPSILON=eps]
 *
 * author: Stefan Buettcher
 * created: 2005-05-31
 * changed: 2005-10-13
 **/


#include "terabyte.h"
#include "../misc/alloc.h"
#include "../misc/assert.h"
#include "../index/compactindex.h"
#include "../index/index_types.h"
#include "../index/index_iterator.h"
#include "../index/index_merger.h"
#include "../index/multiple_index_iterator.h"
#include "../misc/logging.h"
#include <math.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>


#define BUFFER_SIZE (256 * 1024 * 1024)


static char *LOG_ID = "CombineIndices";
static char errorMessage[256];


#define DOCUMENT_START "<doc>"
#define DOCUMENT_END "</doc>"


/** Default Okapi BM25 parameters. **/
static const double OKAPI_K1 = 1.2;
static const double OKAPI_B = 0.50;

/** Base for logarithmic document length scaling. **/
static const double DOC_LENGTH_BASE = 1.05;

/**
 * Minimum relative document size on a logarithmic scale. Everything smaller
 * than this is considered of size MIN_REL_SIZE.
 **/
static const int MIN_REL_SIZE = -127;

/**
 * Maximum relative document size on a logarithmic scale. Everything larger
 * than this is considered of size MAX_REL_SIZE.
 **/
static const int MAX_REL_SIZE = +127;

/** Maximum TF (term frequency) value used for precomputed impact values. **/
static const int MAX_TF_VALUE = 63;


typedef struct {
	offset start;
	unsigned int length;
	int relativeLength;
} DocumentDescriptor;


typedef struct {
	char *term;
	offset documentCount;
	int64_t compressedSize;
} FrequentTerm;


static DocumentDescriptor *documents;
static offset documentCount;
static double avgDocLen;
static offset lastDocumentEnd;
static int prevDocumentPosition = 0;


/**
 * This array tells us for every (relDocLen,TF) pair what Okapi-style impact
 * value we have to expect.
 **/
static int impact[MAX_REL_SIZE - MIN_REL_SIZE + 4][MAX_TF_VALUE + 1];


/**
 * This structure is used to sort postings for the same term by their relative
 * impact values.
 **/
typedef struct {
	offset posting;
	int impact;
} ImpactHeapElement;


static FrequentTerm *termHeap;
static int termHeapSize;


static int compareFrequentTermsByDocumentCountAscending(const void *a, const void *b) {
	FrequentTerm *x = (FrequentTerm*)a;
	FrequentTerm *y = (FrequentTerm*)b;
	if (x->documentCount < y->documentCount)
		return -1;
	else if (x->documentCount > y->documentCount)
		return +1;
	else
		return 0;
} // end of compareFrequentTermsByDocumentCountAscending(const void*, const void*)


static int compareFrequentTermsByTermStringAscending(const void *a, const void *b) {
	FrequentTerm *x = (FrequentTerm*)a;
	FrequentTerm *y = (FrequentTerm*)b;
	return strcmp(x->term, y->term);
} // end of compareFrequentTermsByTermStringAscending(const void*, const void*)


static void mergeIndicesAndCollectMostFrequentTerms(IndexIterator **iterators, int iteratorCount,
		char *outputFile, int numberOfTerms) {

	// allocate memory for the heap that holds the "numberOfTerms" most frequent terms
	termHeapSize = 0;
	termHeap = typed_malloc(FrequentTerm, numberOfTerms + 1);
	for (int i = 0; i < numberOfTerms; i++) {
		termHeap[i].term = typed_malloc(char, MAX_TOKEN_LENGTH + 5);
		termHeap[i].documentCount = 0;
	}

	MultipleIndexIterator *iterator = new MultipleIndexIterator(iterators, iteratorCount);
	CompactIndex *target = new CompactIndex(NULL, outputFile, true);

	// we use "currentTerm" to keep track of the previously processed term;
	// the assumption is that this term will not change in the current iteration,
	// but instead we will only move on to another index
	char currentTerm[MAX_TOKEN_LENGTH * 2];
	currentTerm[0] = 0;
	offset documentsForCurrentTerm = 0;

	const int OUTPUT_BUFFER_SIZE = CompactIndex::TARGET_SEGMENT_SIZE * 5;
	offset outputBuffer[OUTPUT_BUFFER_SIZE];
	int outputBufferPos = 0;

	while (iterator->hasNext()) {
		int lengthDummy, sizeDummy;
		char *nextTerm = iterator->getNextTerm();

#if 0
		// drop all stop words from the final index
		char *nt = nextTerm;
		if (strncmp(nt, "<!>", 3) == 0)
			nt = &nextTerm[3];
		int ntLen = strlen(nt);
		bool dropThisTerm = false;
		if (nt[ntLen - 1] == '$') {
			nt[ntLen - 1] = 0;
			int hashSlot = simpleHashFunction(nt) % STOPWORD_HASHTABLE_SIZE;
			if (stopWordHashTable[hashSlot] > 0)
				dropThisTerm = (strcmp(nt, stopWordList[stopWordHashTable[hashSlot]]) == 0);
			nt[ntLen - 1] = '$';
		}
		else {
			int hashSlot = simpleHashFunction(nt) % STOPWORD_HASHTABLE_SIZE;
			if (stopWordHashTable[hashSlot] > 0)
				dropThisTerm = (strcmp(nt, stopWordList[stopWordHashTable[hashSlot]]) == 0);
		}
		if (dropThisTerm) {
			free(iterator->getNextListCompressed(&lengthDummy, &sizeDummy));
			continue;
		}
#endif

		if (strcmp(nextTerm, currentTerm) != 0) {
			// if the term has changed and we still have postings for the last term in
			// the output buffer, flush them to the target index

			if (outputBufferPos > 0)
				target->addPostings(currentTerm, outputBuffer, outputBufferPos);
			outputBufferPos = 0;

			// check whether this term is one of the frequent guys, in which case we want
			// to put it onto the heap
			bool onlyAlpha = true;
			if (currentTerm[4] != 0) {
				for (int i = 3; currentTerm[i] != 0; i++) {
					if ((currentTerm[i] == '<') || (currentTerm[i] == '>'))
						onlyAlpha = false;
					else if ((currentTerm[i] >= '0') && (currentTerm[i] <= '9'))
						onlyAlpha = false;
				}
			}
			if ((onlyAlpha) && (documentsForCurrentTerm > 0)) {
				if (termHeapSize < numberOfTerms) {
					strcpy(termHeap[termHeapSize].term, currentTerm);
					termHeap[termHeapSize].documentCount = documentsForCurrentTerm;
					if (++termHeapSize >= numberOfTerms)
						qsort(termHeap, termHeapSize, sizeof(FrequentTerm), compareFrequentTermsByDocumentCountAscending);
				}
				else if (documentsForCurrentTerm > termHeap[0].documentCount) {
					// add to existing heap if it is contained in more documents than the
					// currently least frequent term on the heap
					termHeap[0].documentCount = documentsForCurrentTerm;
					strcpy(termHeap[0].term, currentTerm);
					int parent = 0, leftChild = 1, rightChild = 2;
					while (leftChild < termHeapSize) {
						int child = leftChild;
						if (rightChild < termHeapSize)
							if (termHeap[rightChild].documentCount < termHeap[leftChild].documentCount)
								child = rightChild;
						if (termHeap[parent].documentCount <= termHeap[child].documentCount)
							break;
						FrequentTerm temp = termHeap[parent];
						termHeap[parent] = termHeap[child];
						termHeap[child] = temp;
						parent = child;
						leftChild = parent + parent + 1;
						rightChild = parent + parent + 2;
					}
				}
			} // end if ((onlyAlpha) && (documentsForCurrentTerm > 0))

			documentsForCurrentTerm = 0;
			strcpy(currentTerm, nextTerm);
		} // end if (strcmp(nextTerm, currentTerm) != 0)

		PostingListSegmentHeader *header = iterator->getNextListHeader();

		if (header->postingCount + outputBufferPos >= CompactIndex::TARGET_SEGMENT_SIZE * 4) {
			// not enough space: flush buffer first
			target->addPostings(currentTerm, outputBuffer, outputBufferPos);
			outputBufferPos = 0;
		}
		if (((outputBufferPos >= CompactIndex::MIN_MERGE_SEGMENT_SIZE) || (outputBufferPos == 0)) &&
				(header->postingCount >= CompactIndex::MIN_MERGE_SEGMENT_SIZE)) {
			// both the new list and the stuff in the output buffer are big enough
			// to survive on their own, put them directly (compressed) into the new
			// CompactIndex ("target")
			if (outputBufferPos > 0) {
				target->addPostings(currentTerm, outputBuffer, outputBufferPos);
				outputBufferPos = 0;
			}
			int length, size;
			offset first = header->firstElement, last = header->lastElement;
			byte *compressed = iterator->getNextListCompressed(&length, &size, NULL);
			documentsForCurrentTerm += length;
			target->addPostings(currentTerm, compressed, size, length, first, last);
			free(compressed);
		}
		else {
			// if one of them is too small, put the new stuff into the output buffer
			// and wait for more postings for the same term
			int length;
			iterator->getNextListUncompressed(&length, &outputBuffer[outputBufferPos]);
			documentsForCurrentTerm += length;
			outputBufferPos += length;
		}

	} // end while (iterator->hasNext())

	// if we still have some postings in the output buffer, flush them to the target
	if (outputBufferPos > 0) {
		target->addPostings(currentTerm, outputBuffer, outputBufferPos);
		outputBufferPos = 0;
	}

	// sort list of frequent terms by ascending term string
	qsort(termHeap, termHeapSize, sizeof(FrequentTerm), compareFrequentTermsByTermStringAscending);

	offset maxDocuments = 0;
	offset minDocuments = MAX_OFFSET;
	for (int i = 0; i < termHeapSize; i++) {
		if (termHeap[i].documentCount > maxDocuments)
			maxDocuments = termHeap[i].documentCount;
		if (termHeap[i].documentCount < minDocuments)
			minDocuments = termHeap[i].documentCount;
	}

	sprintf(errorMessage, "Most frequent term appears in " OFFSET_FORMAT " documents", maxDocuments);
	log(LOG_DEBUG, LOG_ID, errorMessage);
	sprintf(errorMessage, "N-th-most frequent term appears in " OFFSET_FORMAT " documents", minDocuments);
	log(LOG_DEBUG, LOG_ID, errorMessage);

	target->setGeneration(iterator->getGeneration() + 1);
	target->usedAddressSpace = iterator->getUsedAddressSpace();
	target->deletedAddressSpace = iterator->getDeletedAddressSpace();
	target->smallestOffset = iterator->getSmallestOffset();
	target->biggestOffset = iterator->getBiggestOffset();
	delete iterator;
	delete target;
} // end of mergeIndicesAndCollectMostFrequentTerms(...)


static int weirdnessCount = 0;
static offset thisOne = 0, lastOne = 0;


static int getRelDocumentLengthForPosting(offset posting) {
	// explicitly check whether we are after the last document or whether we have
	// just advanced by 1 document from the last call; in both cases, return immediately

	lastOne = thisOne;
	thisOne = posting;

	unsigned int lower, upper;
	if (documents[prevDocumentPosition + 1].start <= posting) {
		lower = prevDocumentPosition + 1;
		if (documents[lower].start + documents[lower].length > posting) {
			prevDocumentPosition = lower;
			return documents[lower].relativeLength;
		}
	}
	else
		lower = 0;
	if (posting > lastDocumentEnd)
		return MAX_REL_SIZE;

	// find subregion for binary search
	int delta = 1;
	while (lower + delta < documentCount) {
		if (documents[lower + delta].start >= posting)
			break;
		delta += delta;
	}
	upper = lower + delta;
	if (upper > documentCount)
		upper = documentCount;
	lower = lower + ((delta - 1) >> 1);

	// do a binary search in the subregion
	while (upper > lower) {
		unsigned int middle = (lower + upper + 1) >> 1;
		if (documents[middle].start > posting)
			upper = middle - 1;
		else
			lower = middle;
	}

	// print error messages in case we don't like the result; return otherwise
	if ((documents[lower].start > posting) ||
			(documents[lower].start + documents[lower].length <= posting)) {
		sprintf(errorMessage, "This should never happen: Posting outside document: "
				OFFSET_FORMAT, posting);
		log(LOG_ERROR, LOG_ID, errorMessage);
		return MAX_REL_SIZE;
	}
	else {
		if ((lower == prevDocumentPosition) && (lower > 0)) {
			sprintf(errorMessage, "WEIRD: " OFFSET_FORMAT " %d", posting, lower);
			log(LOG_ERROR, LOG_ID, errorMessage);
			weirdnessCount++;
			fprintf(stderr, "lastOne = " OFFSET_FORMAT ", thisOne = " OFFSET_FORMAT "\n", lastOne, thisOne);
			fprintf(stderr, "documentStart = " OFFSET_FORMAT "\n", documents[lower].start);
			fprintf(stderr, "documentLength = %d\n", documents[lower].length);
			fprintf(stderr, "weirdnessCount = %d\n", weirdnessCount);
			prevDocumentPosition = lower;
			return MAX_REL_SIZE;
		}
		prevDocumentPosition = lower;
		return documents[lower].relativeLength;
	}
} // end of getRelDocumentLengthForPosting(offset)


static int getImpactOfPosting(offset posting) {
	int tf = (posting & DOC_LEVEL_MAX_TF);
	int relDocLen = getRelDocumentLengthForPosting(posting);
	return impact[relDocLen - MIN_REL_SIZE][tf];
} // end of getImpactOfPosting(offset)


/** Subroutine of sortHeapByImpact. **/
static void radixSortHeap(ImpactHeapElement *inHeap, int n, int shift, ImpactHeapElement *outHeap) {
	int count[256];
	int pos[256];
	memset(count, 0, sizeof(count));
	for (int i = 0; i < n; i++) {
		int bucket = ((inHeap[i].impact >> shift) & 255);
		count[bucket]++;
	}
	pos[255] = 0;
	for (int i = 254; i >= 0; i--)
		pos[i] = pos[i + 1] + count[i + 1];
	for (int i = 0; i < n; i++) {
		int bucket = ((inHeap[i].impact >> shift) & 255);
		outHeap[pos[bucket]] = inHeap[i];
		pos[bucket]++;
	}
} // end of radixSortHeap(...)


/**
 * Sorts the given array "heap" by increasing impact. This is used to establish the
 * heap property for the array.
 **/
static void sortHeapByImpact(ImpactHeapElement *heap, int heapSize, ImpactHeapElement *temp) {
	for (int i = 0; i < sizeof(int32_t); i++) {
		if ((i & 1) == 0)
			radixSortHeap(heap, heapSize, i * 8, temp);
		else
			radixSortHeap(temp, heapSize, i * 8, heap);
	}
} // end of sortHeapByImpact(ImpactHeapElement*, int, ImpactHeapElement*)


void radixSortPostingsByPosition(offset *inArray, int n, int shift, offset *outArray) {
	int count[256];
	int pos[256];
	memset(count, 0, sizeof(count));
	for (int i = 0; i < n; i++) {
		int bucket = ((inArray[i] >> shift) & 255);
		count[bucket]++;
	}
	pos[0] = 0;
	for (int i = 1; i < 256; i++)
		pos[i] = pos[i - 1] + count[i - 1];
	for (int i = 0; i < n; i++) {
		int bucket = ((inArray[i] >> shift) & 255);
		outArray[pos[bucket]] = inArray[i];
		pos[bucket]++;
	}
} // end of radixSortPostingsByPosition(offset*, int, int, offset*)


/**
 * Calls radixSortPostingsByPosition to perform a radix sort on the given
 * list of postings.
 **/
static void sortPostingsByPositionAscending(offset *postings, int postingCount, offset *temp) {
	for (int i = 0; i < sizeof(offset); i++) {
		if ((i & 1) == 0)
			radixSortPostingsByPosition(postings, postingCount, i * 8, temp);
		else
			radixSortPostingsByPosition(temp, postingCount, i * 8, postings);
	}
} // end of sortPostingsByPositionAscending(offset*, int, offset*)


static void addRestrictedPostingsForTerm(char *term, ExtentList *postings,
		int maxHeapSize, CompactIndex *target) {
	if (postings->getLength() <= 1)
		fprintf(stderr, "Problem with term: \"%s\"\n", term);
	assert(postings->getLength() > 1);

	offset start[256], end[256];
	offset where = 0;
	ImpactHeapElement *heap = typed_malloc(ImpactHeapElement, maxHeapSize + 4);
	ImpactHeapElement *temp = typed_malloc(ImpactHeapElement, maxHeapSize + 4);
	offset *postingsArray = (offset*)temp;
	offset *tempPostingsArray = (offset*)heap;
	int n, heapSize = 0;
	offset documentsForTerm = 0;

	int64_t totalImpactOfAllPostings = 0;

	// use getNextN several times to get all postings; use a heap to sort them
	// by their relative impact
	while ((n = postings->getNextN(where, MAX_OFFSET, 256, start, end)) > 0) {
		assert(start[0] >= where);
		documentsForTerm += n;
		where = start[n - 1] + 1;
		for (int i = 0; i < n; i++) {
			offset p = start[i];
			if (heapSize < maxHeapSize) {
				heap[heapSize].posting = p;
				heap[heapSize].impact = getImpactOfPosting(p);
				totalImpactOfAllPostings += heap[heapSize].impact;
				if (++heapSize >= maxHeapSize)
					sortHeapByImpact(heap, heapSize, temp);
			}
			else {
				int impact = getImpactOfPosting(p);
				totalImpactOfAllPostings += impact;
				if (impact > heap[0].impact) {
					// replace the top element of the heap (posting with least impact) and
					// perform a reheap operation
					int parent = 0, leftChild = 1, rightChild = 2;
					while (leftChild < heapSize) {
						int child = leftChild;
						if (rightChild < heapSize)
							if (heap[rightChild].impact < heap[leftChild].impact)
								child = rightChild;
						if (impact <= heap[child].impact)
							break;
						heap[parent] = heap[child];
						parent = child;
						leftChild = parent + parent + 1;
						rightChild = parent + parent + 2;
					} // end while (leftChild < heapSize)
					heap[parent].posting = p;
					heap[parent].impact = impact;
				}
			}
		}
	} // end while ((n = postings->getNextN(where, MAX_OFFSET, 256, start, end)) > 0)

	assert(documentsForTerm == postings->getLength());

	// add top "maxHeapSize" postings to "target" index
	for (int i = 0; i < heapSize; i++)
		postingsArray[i] = heap[i].posting;
	if (heapSize >= maxHeapSize) {
		// postings are sorted by impact right now; transform into positional ordering
		sortPostingsByPositionAscending(postingsArray, heapSize, tempPostingsArray);
	}

	// finally, insert the postings into the "target" index; don't forget to add
	// a last posting that tells us the original length of the posting list (we need
	// this for computing the term weight at query time)
	postingsArray[heapSize++] = DOCUMENT_COUNT_OFFSET + documentsForTerm;
	int averageImpact = totalImpactOfAllPostings / documentsForTerm;
	postingsArray[heapSize++] = 2 * DOCUMENT_COUNT_OFFSET + averageImpact;
	target->addPostings(term, postingsArray, heapSize);

	free(heap);
	free(temp);
} // end of addRestrictedPostingsForTerm(ExtentList*, int)


static void createSmallIndex(char *inputFile, char *outputFile, int docsPerTerm) {
	CompactIndex *source = new CompactIndex(NULL, inputFile, false);
	CompactIndex *target = new CompactIndex(NULL, outputFile, true);

	ExtentList *docStarts = source->getPostings(DOCUMENT_START, -1);
	ExtentList *docEnds = source->getPostings(DOCUMENT_END, -1);
	ExtentList *docs = new ExtentList_FromTo(docStarts, docEnds);

	// create "documents" array
	documentCount = docs->getLength();
	documents = typed_malloc(DocumentDescriptor, documentCount + 1);
	offset start = -1, end;
	double totalLength = 0.0;
	for (int i = 0; i < documentCount; i++) {
		docs->getFirstStartBiggerEq(start + 1, &start, &end);
		if (end > start + 1000000000)
			end = start + 1000000000;
		documents[i].start = start;
		documents[i].length = (unsigned int)(end - start + 1);
		totalLength += documents[i].length;
	}
	documents[documentCount].start = MAX_OFFSET;
	avgDocLen = totalLength / documentCount;
	delete docs;
	
	// compute relative lengths for all documents
	for (int i = 0; i < documentCount; i++) {
		double relLen = documents[i].length / avgDocLen;
		int relLenInt = lround(log(relLen) / log(DOC_LENGTH_BASE));
		if (relLenInt > MAX_REL_SIZE)
			relLenInt = MAX_REL_SIZE;
		else if (relLenInt < -MIN_REL_SIZE)
			relLenInt = -MIN_REL_SIZE;
		documents[i].relativeLength = relLenInt;
	}

	lastDocumentEnd =
		(documents[documentCount - 1].start + documents[documentCount - 1].length) - 1;

	// precompute impact values for realistic (avgDocLen, TF) pairs
	for (int docLen = MIN_REL_SIZE; docLen <= MAX_REL_SIZE; docLen++) {
		double relLen = pow(DOC_LENGTH_BASE, docLen);
		double K = OKAPI_K1 * ((1 - OKAPI_B) + OKAPI_B * relLen);
		for (int tf = 0; tf <= MAX_TF_VALUE; tf++) {
			double TF = decodeDocLevelTF(tf);
			double impactHere = TF * (1.0 + OKAPI_K1) / (TF + K);
			impact[docLen - MIN_REL_SIZE][tf] = lround(impactHere * IMPACT_INTEGER_SCALING);
		}
	}

	ImpactHeapElement *impactHeap = typed_malloc(ImpactHeapElement, docsPerTerm + 1);
	int impactHeapSize;

	// do the impact restrictions for all terms in the list
	for (int i = 0; i < termHeapSize; i++) {
		ExtentList *postings = source->getPostings(termHeap[i].term, -1);
		addRestrictedPostingsForTerm(termHeap[i].term, postings, docsPerTerm, target);
		delete postings;
	}

	free(impactHeap);
	free(documents);

	delete source;
	delete target;
} // end of createSmallIndex(char*, int)


int main(int argc, char **argv) {
	struct stat buf;

	if (argc < 3) {
		fprintf(stderr, "Syntax:  combine_indices INPUT_1 .. INPUT_N OUTPUT_LARGE OUTPUT_SMALL N=n K=k\n");
		return 1;
	}
	if ((strncmp(argv[argc - 2], "N=", 2) != 0) || (strncmp(argv[argc - 1], "K=", 2) != 0)) {
		fprintf(stderr, "Syntax:  combine_indices INPUT_1 .. INPUT_N OUTPUT_LARGE OUTPUT_SMALL N=n K=k\n");
		return 1;
	}
	int N = atoi(&argv[argc - 2][2]);
	int K = atoi(&argv[argc - 1][2]);

	char *largeOutputFile = argv[argc - 4];
	if (stat(largeOutputFile, &buf) == 0) {
		fprintf(stderr, "Error: Output index \"%s\" already exists. Program terminated.\n", largeOutputFile);
		return 1;
	}

	char *smallOutputFile = argv[argc - 3];
	if (stat(smallOutputFile, &buf) == 0) {
		fprintf(stderr, "Error: Output index \"%s\" already exists. Program terminated.\n", smallOutputFile);
		return 1;
	}

	IndexIterator **iterators = typed_malloc(IndexIterator*, argc);
	int inputCount = argc - 5;
	for (int i = 0; i < inputCount; i++) {
		char *inputFile = argv[i + 1];
		iterators[i] = new IndexIterator(inputFile, BUFFER_SIZE / inputCount);
	}

	initializeStopWordHashtable();

	mergeIndicesAndCollectMostFrequentTerms(iterators, inputCount, largeOutputFile, N);

	log(LOG_DEBUG, LOG_ID, "Indices merged and frequent terms collected.");
	sprintf(errorMessage, "Sorting postings for %d most frequent terms by their impact...", termHeapSize);
	log(LOG_DEBUG, LOG_ID, errorMessage);

	createSmallIndex(largeOutputFile, smallOutputFile, K);
	
	return 0;
} // end of main(int, char**)


